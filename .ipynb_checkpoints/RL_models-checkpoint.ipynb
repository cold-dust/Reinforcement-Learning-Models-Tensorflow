{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-26 19:15:55,070] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    # Q-learning Algoithm\n",
    "    while j<99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s, a] = Q[s, a] + lr*(r + y*np.max(Q[s1, :]) - Q[s, a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d==True:\n",
    "            break\n",
    "            \n",
    "    jList.append(j)\n",
    "    rList.append(rAll)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(jList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Q-Network Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape = [1,4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "#set Learning parameters\n",
    "y = .99\n",
    "e = 0.1\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        # Q-Network\n",
    "        while j<99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a, allQ = sess.run([predict, Qout], feed_dict = {inputs1:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            #Get new state and reward from environment\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a[0]] = r + y*maxQ1\n",
    "            \n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d==True:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(jList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-armed bandit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandits = [0.3, 0, -0.3, 0.5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    #Get a random number.\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        #return a positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        #return a negative reward.\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "reward_holder = tf.placeholder(shape = [1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape = [1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i=0\n",
    "    while i<total_episodes:\n",
    "        \n",
    "        if np.random.rand(1)<e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        #Update the network\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights], feed_dict={reward_holder:[reward],\n",
    "                                                                            action_holder:[action]})\n",
    "        print \"ww:\", ww\n",
    "        print \"reward: \", reward\n",
    "        print \"action: \", action\n",
    "        \n",
    "        print \"loss: \",-(np.log(resp[0])*reward)\n",
    "        #Update our running tally of scores. \n",
    "        total_reward[action] += reward\n",
    "        if i%50 == 0:\n",
    "            print \"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward)\n",
    "        i+=1\n",
    "        print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "        \n",
    "        if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "            print \"...and it was right!\"\n",
    "        else:\n",
    "            print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Contextual Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0, self.num_bandits)\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            #return a positive reward.\n",
    "            return 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
    "        output = slim.fully_connected(state_in_OH, a_size, biases_initializer=None,\n",
    "                                     activation_fn = tf.nn.sigmoid,\n",
    "                                     weights_initializer = tf.ones_initializer())\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        \n",
    "        self.reward_holder = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape = [1], dtype = tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cse/ug/14075009/virtualenv-1.9/myVE/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-26 19:19:59,749] From /home/cse/ug/14075009/virtualenv-1.9/myVE/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [ 0.    0.    0.25]\n",
      "Mean reward for each of the 3 bandits: [  1.75  36.25  36.75]\n",
      "Mean reward for each of the 3 bandits: [ 10.75  73.25  74.25]\n",
      "Mean reward for each of the 3 bandits: [   8.75  108.5   112.  ]\n",
      "Mean reward for each of the 3 bandits: [  14.    148.    145.75]\n",
      "Mean reward for each of the 3 bandits: [  14.75  189.5   177.  ]\n",
      "Mean reward for each of the 3 bandits: [  19.    226.    209.25]\n",
      "Mean reward for each of the 3 bandits: [  20.25  264.25  246.25]\n",
      "Mean reward for each of the 3 bandits: [  17.    297.75  286.  ]\n",
      "Mean reward for each of the 3 bandits: [  41.5   335.25  319.  ]\n",
      "Mean reward for each of the 3 bandits: [  78.75  375.    355.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 121.5   408.75  390.  ]\n",
      "Mean reward for each of the 3 bandits: [ 165.5   445.75  422.  ]\n",
      "Mean reward for each of the 3 bandits: [ 203.    478.5   463.25]\n",
      "Mean reward for each of the 3 bandits: [ 242.    513.75  498.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 278.75  550.25  537.75]\n",
      "Mean reward for each of the 3 bandits: [ 314.75  588.5   572.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 357.25  628.75  603.75]\n",
      "Mean reward for each of the 3 bandits: [ 400.25  665.25  635.25]\n",
      "Mean reward for each of the 3 bandits: [ 439.    704.75  667.5 ]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cBandit = contextual_bandit()\n",
    "myAgent = agent(lr=0.001, s_size = cBandit.num_bandits, a_size = cBandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
    "e=0.1\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i=0\n",
    "    while i < total_episodes:\n",
    "        s = cBandit.getBandit()\n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict={myAgent.state_in:[s]})\n",
    "        \n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "        #Update the network.\n",
    "        \n",
    "        _, ww = sess.run([myAgent.update,weights], feed_dict={myAgent.reward_holder:[reward],\n",
    "                                    myAgent.action_holder:[action],\n",
    "                                    myAgent.state_in:[s]} )\n",
    "        #Update our running tally of scores.\n",
    "        total_reward[s,action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print \"Mean reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1))\n",
    "        i+=1\n",
    "        \n",
    "for a in range(cBandit.num_bandits):\n",
    "    print \"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\"\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print \"...and it was right!\"\n",
    "    else:\n",
    "        print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Vanilla Policy Gradient Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-30 16:12:32,670] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Policy-Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_reward(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = r[t] + running_add*gamma\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        #These lines established the feed-forward part of the network.\n",
    "        #The agent takes a state and produces an action.\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in, h_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden, a_size, activation_fn = tf.nn.softmax, biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        #The next six lines establish the training proceedure.\n",
    "        #We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cse/ug/14075009/virtualenv-1.9/myVE/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-30 16:13:53,859] From /home/cse/ug/14075009/virtualenv-1.9/myVE/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "myAgent = agent(lr=1e-2, s_size=4, a_size=2, h_size=8)\n",
    "\n",
    "total_episodes = 5000\n",
    "max_ep = 999\n",
    "update_frequecy = 5\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i=0\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad*0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(myAgent.output, feed_dict = {myAgent.state_in:[s]})\n",
    "            a = np.random.choice(a_dist[0], p = a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "            \n",
    "            env.render()\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            ep_history.append([s,a,r,s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            \n",
    "            if d==True:\n",
    "                #Update the network\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 2] = discount_reward(ep_history[:,2])\n",
    "                feed_dict = {myAgent.reward_holder:ep_history[:,2], \n",
    "                            myAgent.action_holder:ep_history[:, 1],\n",
    "                            myAgent.state_in:np.vstack(ep_history[:, 0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "                \n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "                \n",
    "                if i%update_frequecy==0 and i!=0:\n",
    "                    feed_dict = dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict = feed_dict)\n",
    "                    for ix, grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad*0\n",
    "                    \n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "                break\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "        i += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model - 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Network for Agent policy\n",
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None, 4], name = \"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4,H], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name = \"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name = \"reward_signal\")\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "W1Grad = tf.placeholder(tf.float32, name = \"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32, name = \"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "\n",
    "loglik = tf.log(input_y*(input_y-probability) + (1-input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik*advantages)\n",
    "newGrads = tf.gradients(loss, tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here we implement a multi-layer neural network that predicts the next observation,\n",
    "#reward, and done state from a current state and action.\n",
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat(1,[predicted_observation,predicted_reward,predicted_done])\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.mul(predicted_done, true_done) + tf.mul(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Helper-functions\n",
    "In [9]:\n",
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done\n",
    "Training the Policy and Model\n",
    "In [10]:\n",
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.initialize_all_variables()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print 'World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs)\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print real_episodes\n",
    "World Perf: Episode 4.000000. Reward 34.666667. action: 1.000000. mean reward 34.666667.\n",
    "World Perf: Episode 7.000000. Reward 18.333333. action: 1.000000. mean reward 34.503333.\n",
    "World Perf: Episode 10.000000. Reward 30.666667. action: 1.000000. mean reward 34.464967.\n",
    "World Perf: Episode 13.000000. Reward 34.333333. action: 0.000000. mean reward 34.463650.\n",
    "World Perf: Episode 16.000000. Reward 39.666667. action: 0.000000. mean reward 34.515680.\n",
    "World Perf: Episode 19.000000. Reward 20.000000. action: 0.000000. mean reward 34.370524.\n",
    "World Perf: Episode 22.000000. Reward 30.333333. action: 0.000000. mean reward 34.330152.\n",
    "World Perf: Episode 25.000000. Reward 20.333333. action: 1.000000. mean reward 34.190184.\n",
    "World Perf: Episode 28.000000. Reward 19.333333. action: 1.000000. mean reward 34.041615.\n",
    "World Perf: Episode 31.000000. Reward 32.333333. action: 0.000000. mean reward 34.024532.\n",
    "World Perf: Episode 34.000000. Reward 26.333333. action: 0.000000. mean reward 33.947620.\n",
    "World Perf: Episode 37.000000. Reward 25.000000. action: 0.000000. mean reward 33.858144.\n",
    "World Perf: Episode 40.000000. Reward 41.000000. action: 1.000000. mean reward 33.929563.\n",
    "World Perf: Episode 43.000000. Reward 26.333333. action: 1.000000. mean reward 33.853600.\n",
    "World Perf: Episode 46.000000. Reward 25.333333. action: 0.000000. mean reward 33.768398.\n",
    "World Perf: Episode 49.000000. Reward 22.000000. action: 0.000000. mean reward 33.650714.\n",
    "World Perf: Episode 52.000000. Reward 35.666667. action: 1.000000. mean reward 33.670873.\n",
    "World Perf: Episode 55.000000. Reward 30.000000. action: 1.000000. mean reward 33.634165.\n",
    "World Perf: Episode 58.000000. Reward 29.000000. action: 1.000000. mean reward 33.587823.\n",
    "World Perf: Episode 61.000000. Reward 31.666667. action: 1.000000. mean reward 33.568611.\n",
    "World Perf: Episode 64.000000. Reward 29.000000. action: 0.000000. mean reward 33.522925.\n",
    "World Perf: Episode 67.000000. Reward 27.000000. action: 0.000000. mean reward 33.457696.\n",
    "World Perf: Episode 70.000000. Reward 58.333333. action: 1.000000. mean reward 33.706452.\n",
    "World Perf: Episode 73.000000. Reward 29.333333. action: 0.000000. mean reward 33.662721.\n",
    "World Perf: Episode 76.000000. Reward 12.333333. action: 0.000000. mean reward 33.449427.\n",
    "World Perf: Episode 79.000000. Reward 25.000000. action: 0.000000. mean reward 33.364933.\n",
    "World Perf: Episode 82.000000. Reward 21.666667. action: 1.000000. mean reward 33.247950.\n",
    "World Perf: Episode 85.000000. Reward 27.333333. action: 0.000000. mean reward 33.188804.\n",
    "World Perf: Episode 88.000000. Reward 34.000000. action: 1.000000. mean reward 33.196916.\n",
    "World Perf: Episode 91.000000. Reward 22.333333. action: 0.000000. mean reward 33.088280.\n",
    "World Perf: Episode 94.000000. Reward 43.000000. action: 1.000000. mean reward 33.187397.\n",
    "World Perf: Episode 97.000000. Reward 14.000000. action: 0.000000. mean reward 32.995523.\n",
    "World Perf: Episode 100.000000. Reward 19.000000. action: 1.000000. mean reward 32.855568.\n",
    "World Perf: Episode 103.000000. Reward 23.333333. action: 1.000000. mean reward 32.760346.\n",
    "World Perf: Episode 106.000000. Reward 41.666667. action: 0.000000. mean reward 32.733059.\n",
    "World Perf: Episode 109.000000. Reward 48.000000. action: 1.000000. mean reward 32.898281.\n",
    "World Perf: Episode 112.000000. Reward 17.000000. action: 1.000000. mean reward 33.378304.\n",
    "World Perf: Episode 115.000000. Reward 52.333333. action: 0.000000. mean reward 33.407272.\n",
    "World Perf: Episode 118.000000. Reward 16.000000. action: 1.000000. mean reward 32.948177.\n",
    "World Perf: Episode 121.000000. Reward 22.000000. action: 0.000000. mean reward 32.596844.\n",
    "World Perf: Episode 124.000000. Reward 30.000000. action: 1.000000. mean reward 32.334019.\n",
    "World Perf: Episode 127.000000. Reward 22.666667. action: 1.000000. mean reward 31.985229.\n",
    "World Perf: Episode 130.000000. Reward 75.333333. action: 1.000000. mean reward 32.219444.\n",
    "World Perf: Episode 133.000000. Reward 37.666667. action: 0.000000. mean reward 32.033123.\n",
    "World Perf: Episode 136.000000. Reward 34.666667. action: 1.000000. mean reward 36.278812.\n",
    "World Perf: Episode 139.000000. Reward 44.333333. action: 0.000000. mean reward 37.793564.\n",
    "World Perf: Episode 142.000000. Reward 43.000000. action: 0.000000. mean reward 37.605556.\n",
    "World Perf: Episode 145.000000. Reward 17.333333. action: 0.000000. mean reward 37.140411.\n",
    "World Perf: Episode 148.000000. Reward 58.000000. action: 0.000000. mean reward 37.012058.\n",
    "World Perf: Episode 151.000000. Reward 54.000000. action: 0.000000. mean reward 36.910122.\n",
    "World Perf: Episode 154.000000. Reward 43.666667. action: 0.000000. mean reward 36.677944.\n",
    "World Perf: Episode 157.000000. Reward 49.666667. action: 1.000000. mean reward 36.497761.\n",
    "World Perf: Episode 160.000000. Reward 33.333333. action: 1.000000. mean reward 36.195156.\n",
    "World Perf: Episode 163.000000. Reward 26.333333. action: 0.000000. mean reward 35.835846.\n",
    "World Perf: Episode 166.000000. Reward 36.333333. action: 0.000000. mean reward 35.844830.\n",
    "World Perf: Episode 169.000000. Reward 28.333333. action: 1.000000. mean reward 35.657898.\n",
    "World Perf: Episode 172.000000. Reward 47.333333. action: 1.000000. mean reward 35.645672.\n",
    "World Perf: Episode 175.000000. Reward 54.000000. action: 1.000000. mean reward 35.669083.\n",
    "World Perf: Episode 178.000000. Reward 39.666667. action: 1.000000. mean reward 35.657860.\n",
    "World Perf: Episode 181.000000. Reward 29.000000. action: 0.000000. mean reward 36.254162.\n",
    "World Perf: Episode 184.000000. Reward 40.333333. action: 1.000000. mean reward 38.040600.\n",
    "World Perf: Episode 187.000000. Reward 47.000000. action: 1.000000. mean reward 39.061661.\n",
    "World Perf: Episode 190.000000. Reward 50.666667. action: 0.000000. mean reward 40.936543.\n",
    "World Perf: Episode 193.000000. Reward 41.666667. action: 1.000000. mean reward 41.026863.\n",
    "World Perf: Episode 196.000000. Reward 39.666667. action: 0.000000. mean reward 42.870926.\n",
    "World Perf: Episode 199.000000. Reward 40.333333. action: 1.000000. mean reward 43.584274.\n",
    "World Perf: Episode 202.000000. Reward 37.333333. action: 0.000000. mean reward 43.347317.\n",
    "World Perf: Episode 205.000000. Reward 40.333333. action: 0.000000. mean reward 44.021179.\n",
    "World Perf: Episode 208.000000. Reward 21.333333. action: 1.000000. mean reward 43.482227.\n",
    "World Perf: Episode 211.000000. Reward 41.666667. action: 0.000000. mean reward 43.259212.\n",
    "World Perf: Episode 214.000000. Reward 34.000000. action: 1.000000. mean reward 44.416565.\n",
    "World Perf: Episode 217.000000. Reward 34.666667. action: 0.000000. mean reward 44.088322.\n",
    "World Perf: Episode 220.000000. Reward 34.666667. action: 1.000000. mean reward 44.904999.\n",
    "World Perf: Episode 223.000000. Reward 47.333333. action: 1.000000. mean reward 44.721806.\n",
    "World Perf: Episode 226.000000. Reward 44.666667. action: 1.000000. mean reward 45.130310.\n",
    "World Perf: Episode 229.000000. Reward 43.333333. action: 0.000000. mean reward 44.777817.\n",
    "World Perf: Episode 232.000000. Reward 43.333333. action: 0.000000. mean reward 44.769470.\n",
    "World Perf: Episode 235.000000. Reward 44.333333. action: 0.000000. mean reward 46.332897.\n",
    "World Perf: Episode 238.000000. Reward 45.000000. action: 1.000000. mean reward 45.947269.\n",
    "World Perf: Episode 241.000000. Reward 54.666667. action: 0.000000. mean reward 46.343395.\n",
    "World Perf: Episode 244.000000. Reward 24.000000. action: 1.000000. mean reward 45.764530.\n",
    "World Perf: Episode 247.000000. Reward 34.333333. action: 1.000000. mean reward 45.315842.\n",
    "World Perf: Episode 250.000000. Reward 48.333333. action: 0.000000. mean reward 45.853481.\n",
    "World Perf: Episode 253.000000. Reward 67.333333. action: 0.000000. mean reward 45.776394.\n",
    "World Perf: Episode 256.000000. Reward 34.000000. action: 1.000000. mean reward 47.594440.\n",
    "World Perf: Episode 259.000000. Reward 38.333333. action: 0.000000. mean reward 47.181637.\n",
    "World Perf: Episode 262.000000. Reward 66.000000. action: 0.000000. mean reward 47.100784.\n",
    "World Perf: Episode 265.000000. Reward 34.333333. action: 0.000000. mean reward 46.671185.\n",
    "World Perf: Episode 268.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
